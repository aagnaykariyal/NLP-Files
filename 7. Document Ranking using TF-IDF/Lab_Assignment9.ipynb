{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name: Aagnay Kariyal\n",
    "#### ID: 8830232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# Importing the required dependencies\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:57:50.275469Z",
     "start_time": "2023-11-18T17:57:50.271004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T17:57:50.313997Z",
     "start_time": "2023-11-18T17:57:50.287625Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "## TODO: Clean the sentences\n",
    "def nlp_lem(t): # Creating a function to do lemmatization and POS tagging\n",
    "    tokens = word_tokenize(t) # Tokenization\n",
    "    # Removing stop words\n",
    "    stop_words_removed = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Creating a list of punctuations to remove\n",
    "    punctuation_list = list(string.punctuation)\n",
    "    # Saving the punctuations removed words into a list\n",
    "    punctuation_removed = [word for word in stop_words_removed if word not in punctuation_list]\n",
    "    # Creating a list to save all the double punctuation errors\n",
    "    double_punkt = []\n",
    "    # Creating a for loop to create the double punctuation errors so that we can cross-check with the main data to remove the errors from the main data.\n",
    "    for w in punctuation_list:\n",
    "        double_punkt.append(w+w)\n",
    "    # We save the cleaned data by checking for the error punctuations present inside the main data\n",
    "    punctuation_removed = [word for word in punctuation_removed if word not in double_punkt]\n",
    "    # Changing the case inside the list to normalize it\n",
    "    case_folded = [word.lower() for word in punctuation_removed]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in case_folded]\n",
    "    \n",
    "    # POS Tagging and returning the values\n",
    "    # return nltk.pos_tag(word_tokenize(\" \".join(lemmatized_words)))\n",
    "    return lemmatized_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T17:57:53.211005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# Creating a function to lemmatize without removing stop words\n",
    "def lemma(l):\n",
    "    tokens = word_tokenize(l) # Tokenization\n",
    "    # Creating a list of punctuations to remove\n",
    "    punctuation_list = list(string.punctuation)\n",
    "    # Saving the punctuations removed words into a list\n",
    "    punctuation_removed = [word for word in tokens if word not in punctuation_list]\n",
    "    # Creating a list to save all the double punctuation errors\n",
    "    double_punkt = []\n",
    "    # Creating a for loop to create the double punctuation errors so that we can cross-check with the main data to remove the errors from the main data.\n",
    "    for w in punctuation_list:\n",
    "        double_punkt.append(w+w)\n",
    "    # We save the cleaned data by checking for the error punctuations present inside the main data\n",
    "    punctuation_removed = [word for word in punctuation_removed if word not in double_punkt]\n",
    "    # Changing the case inside the list to normalize it\n",
    "    case_folded = [word.lower() for word in punctuation_removed]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in case_folded]\n",
    "    \n",
    "    # POS Tagging and returning the values\n",
    "    return lemmatized_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T04:58:05.961717Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Creating a dictionary consisting of sentences and document ids\n",
    "doc_id = {}\n",
    "i = 0\n",
    "\n",
    "for sen in sample_sentences:\n",
    "    i = i+1  # Incrementing the value to assign the number to a sentence/document\n",
    "    doc_id[sen] = i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-18T17:57:55.888489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': [1], 'versatile': [1], 'programming': [1, 4, 5, 10], 'language': [1]} \n",
      "\n",
      "{'javascript': [2], 'widely': [2], 'used': [2], 'web': [2, 9], 'development': [2, 9]} \n",
      "\n",
      "{'java': [2, 3], 'known': [3], 'platform': [3], 'independence': [3]} \n",
      "\n",
      "{'programming': [1, 4, 5, 10], 'involves': [4], 'writing': [4], 'code': [4, 7, 8], 'solve': [4], 'problem': [4, 6]} \n",
      "\n",
      "{'data': [5], 'structure': [5], 'crucial': [5], 'efficient': [5], 'programming': [1, 4, 5, 10]} \n",
      "\n",
      "{'algorithm': [6], 'step-by-step': [6], 'instruction': [6], 'solving': [6], 'problem': [4, 6]} \n",
      "\n",
      "{'version': [7], 'control': [7], 'system': [7], 'help': [7], 'manage': [7], 'code': [4, 7, 8], 'change': [7], 'collaboration': [7]} \n",
      "\n",
      "{'debugging': [8], 'process': [8], 'finding': [8], 'fixing': [8], 'error': [8], 'code': [4, 7, 8]} \n",
      "\n",
      "{'web': [2, 9], 'framework': [9], 'simplify': [9], 'development': [2, 9], 'application': [9]} \n",
      "\n",
      "{'artificial': [10], 'intelligence': [10], 'applied': [10], 'various': [10], 'programming': [1, 4, 5, 10], 'task': [10]} \n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(sam):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "    terms = nlp_lem(sam)  # Getting a list of all the tokenized terms of the particular sentence\n",
    "    \n",
    "    inv_ind = {}\n",
    "    for t in terms:  # Iterating through each term in terms\n",
    "        li = []\n",
    "        for sen in sample_sentences:\n",
    "            if t in sen.lower():  # Checking if each term is in each document one by one\n",
    "                id = doc_id[sen]  \n",
    "                li.append(id)  # If the term is present inside the document, the id of the document gets saved into a list called li\n",
    "        inv_ind[t] = li\n",
    "\n",
    "## The for loop above takes one term and finds the documents for that term and does the same for all the terms present in that document\n",
    "    return inv_ind\n",
    "\n",
    "for s in sample_sentences:  # Iterating through all the sentences in the corpus to get the inverted index of all the sentences\n",
    "    print(get_inverted_index(s) ,'\\n')\n",
    "# get_inverted_index(sample_sentences[4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T04:58:05.680538Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T22:04:10.478347Z",
     "start_time": "2023-11-18T22:04:10.297847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          Terms  Doc_ID  Position\n0    artificial      10         1\n1  intelligence      10         2\n2       applied      10         5\n3       various      10         7\n4   programming       1         5\n5   programming       4         1\n6   programming       5         7\n7   programming      10         8\n8          task      10         9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Terms</th>\n      <th>Doc_ID</th>\n      <th>Position</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>artificial</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>intelligence</td>\n      <td>10</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>applied</td>\n      <td>10</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>various</td>\n      <td>10</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>programming</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>programming</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>programming</td>\n      <td>5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>programming</td>\n      <td>10</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>task</td>\n      <td>10</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_positional_index(s):\n",
    "## TODO: Implement the functionality that will return the positional index\n",
    "    for sentence in s:\n",
    "        # Creating a list of tokenized terms\n",
    "        terms = nlp_lem(sentence)\n",
    "        \n",
    "        # Creating lists to store the terms, doc_id and position of the term in that document\n",
    "        term_list = []\n",
    "        doc_list = []\n",
    "        pos_list = []\n",
    "        \n",
    "        positional_index = pd.DataFrame(columns=['Terms', 'Doc_ID', 'Position']) # Creating a pandas dataframe for better representation\n",
    "    \n",
    "        for t in terms:     # Iterating through all the terms in the list of tokenized terms\n",
    "            for sen in sample_sentences:  # Iterating through all the sentences in the corpus\n",
    "                split = lemma(sen)  # Getting all the words split including stop words to get the position index\n",
    "                sent_val = {}  # Dictionary to store the term and their positional index\n",
    "                j = 0  # A variable to store the positional values\n",
    "                for s in split:  # Iterating through all the words in the list that includes the stopwords\n",
    "                    j = j+1  # Positional value is incremented as the loop goes on\n",
    "                    sent_val[s] = j # Inserting the term and their respective position index into a dictionary\n",
    "                if t in sent_val.keys():  # If the tokenized term is inside the dictionary, then append the term, the doc_id and the position to their respective lists\n",
    "                    pos_dict = {'Terms': t, 'Doc_ID': doc_id[sen], 'Position':sent_val[t]} # Creating a dictionary that consists of all the values in their respective lists\n",
    "                    positional_index.loc[len(positional_index)] = pos_dict\n",
    "                    # positional_index.append(pos_dict, ignore_index=True)\n",
    "                    # positional_index.loc[len(positional_index)] = [t, doc_id[sen], sent_val[t]]\n",
    "    \n",
    "    return positional_index # Returning the pandas dataframe\n",
    "\n",
    "get_positional_index(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "                Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  \\\npython       0.698970    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nis           0.301030    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \na           -0.041393    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nversatile    0.698970    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nprogramming  0.397940    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...               ...    ...    ...    ...    ...    ...    ...    ...    ...   \napplied      0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nin           0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nvarious      0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \nprogramming  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \ntasks        0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n               Doc 10  \npython       0.000000  \nis           0.000000  \na            0.000000  \nversatile    0.000000  \nprogramming  0.000000  \n...               ...  \napplied      0.698970  \nin           0.045757  \nvarious      0.698970  \nprogramming  0.397940  \ntasks        0.698970  \n\n[78 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Doc 1</th>\n      <th>Doc 2</th>\n      <th>Doc 3</th>\n      <th>Doc 4</th>\n      <th>Doc 5</th>\n      <th>Doc 6</th>\n      <th>Doc 7</th>\n      <th>Doc 8</th>\n      <th>Doc 9</th>\n      <th>Doc 10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>python</th>\n      <td>0.698970</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>is</th>\n      <td>0.301030</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>-0.041393</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>versatile</th>\n      <td>0.698970</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>programming</th>\n      <td>0.397940</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>applied</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.698970</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.045757</td>\n    </tr>\n    <tr>\n      <th>various</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.698970</td>\n    </tr>\n    <tr>\n      <th>programming</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.397940</td>\n    </tr>\n    <tr>\n      <th>tasks</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.698970</td>\n    </tr>\n  </tbody>\n</table>\n<p>78 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_idf(term, documents):\n",
    "    n_docs_with_term = sum(1 for doc in documents if term in doc) # calculating the number of documents with the term\n",
    "    idf = math.log10(len(documents) / (1 + n_docs_with_term))  # Calculating the idf\n",
    "    return idf  # Returning just the idf\n",
    "    \n",
    "def get_tfidf(sentence, corpus):\n",
    "    words = sentence.split() # Creating a list of words\n",
    "\n",
    "    # Creating lists to store the tfidf scores and terms\n",
    "    tfidf_scores = []\n",
    "    terms = []\n",
    "    \n",
    "    d_id = doc_id[sentence]  # Getting the document id of the sentence passed through \n",
    "    \n",
    "    for word in words:  # Iterating through all the words in the sentence\n",
    "        terms.append(word.lower().replace('.',''))  # Removing full stops and normalizing the terms before appending all the words to a list called terms\n",
    "        tf = words.count(word) # Calculating the term frequency\n",
    "        idf = get_idf(word, corpus) # Calculating the inverse document frequency from the function created above\n",
    "        tfidf = tf * idf # Calculating the TF-IDF score\n",
    "        tfidf_scores.append(tfidf) # Appending the tfidf scores into a list\n",
    "\n",
    "    return d_id, tfidf_scores, terms  # Returning the document id, tfidf scores and the terms lists\n",
    "\n",
    "def get_tfidf_matrix(ss):\n",
    "    \n",
    "    # Creating lists to save document ids, tf_idf scores and terms\n",
    "    docs_ids = []\n",
    "    tf_idf_scores_list = []\n",
    "    terms_list = []\n",
    "    \n",
    "    for sam in ss: # Iterating through all the sentences in the corpus\n",
    "        results = get_tfidf(sam, ss)  # Getting the tfidf scores using the sentence in the corpus\n",
    "        docs_ids.append('Doc ' + str(results[0])) # Inserting the document ids of the particular sentence in the doc_ids list\n",
    "    \n",
    "        for val in range(len(results[2])): # Iterating through all the terms\n",
    "            terms_list.append(results[2][val])  # Appending all the terms in the sentence into a terms list\n",
    "    \n",
    "    total_values = len(terms_list) # Getting the length of the terms list\n",
    "    \n",
    "    null_vals_before = 0   # Creating a count of the number of null values to be put in before the list of tfidf values\n",
    "    \n",
    "    for sam in ss:  # For the sentence in the corpus\n",
    "        null = [0]  # creating a list of null values to be repeated \n",
    "        null_to_add_before = null * null_vals_before  # Getting the list of null values to be added before tfidf values\n",
    "        results = get_tfidf(sam, ss) # Getting the tfidf values\n",
    "        null_vals_after = total_values - len(results[1]) - null_vals_before  # Getting the number of null values to be added after the list of tfidf values\n",
    "    \n",
    "        null_vals_before = null_vals_before + len(results[1]) # Calculating the number of null values to be added before the list of tfidf values\n",
    "        null_to_add_after = null * null_vals_after  # Getting the list of null values to be added after tfidf values\n",
    "        tf_idf_scores_list.append(null_to_add_before+results[1]+null_to_add_after)  # Creating a column consisting of null values and tfidf scores\n",
    "    \n",
    "    zip_list = zip(docs_ids, tf_idf_scores_list) # Creating a list to be iterated into a dictionary\n",
    "    tf_idf_dict = dict(zip_list)  # Creating a dictionary to be iterated into a pandas dataframe\n",
    "    tf_idf_df = pd.DataFrame(data=tf_idf_dict, index=terms_list)  # Creating a pandas dataframe for easy understanding \n",
    "    return tf_idf_df  # Returning a tfidf matrix\n",
    "\n",
    "get_tfidf_matrix(sample_sentences)  # Checking to see the output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T22:03:48.149288Z",
     "start_time": "2023-11-18T22:03:48.081268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T22:03:52.935408Z",
     "start_time": "2023-11-18T22:03:52.507873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 0.6989700043360189, 5: 0.6989700043360189, 1: 0.3979400086720376, 10: 0.3979400086720376, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n"
     ]
    }
   ],
   "source": [
    "def get_ranked_documents(sample_sentences, search_query, method_name='tfidf'):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    if method_name == 'tfidf': # Checking the method\n",
    "        t_df = get_tfidf_matrix(sample_sentences) # Storing the tfidf matrx in a variable\n",
    "        sums = {} # Creating a dict to store all the ranks and sentences \n",
    "        \n",
    "        for ids in doc_id: # Iterating through each id in doc_ids\n",
    "            sum = t_df.loc[search_query, 'Doc '+str(doc_id[ids])].sum() # Getting the sum of all the tfidf scores of the particular document according to the search query\n",
    "            sums[doc_id[ids]] = sum # Inserting the document id and the sum into a dictionary\n",
    "            \n",
    "        ranked_list = dict(sorted(sums.items(), key=lambda x: x[1], reverse=True)) # Creating a ranked list of all the document ids and their respective tfidf ranks\n",
    "        return ranked_list # Returning the list of the documents sorted according to their ranks of tfidf scores\n",
    "    \n",
    "    elif method_name == 'inverted':  # Checking the method\n",
    "        q = lemma(search_query)[0] # Saving the lemmatized term into a variable\n",
    "        inv_ind = [] # Creating a list to save all the inverted index values\n",
    "        \n",
    "        for s in sample_sentences: # Iterating through all the sentences in the corpus\n",
    "            inv_ind.append(get_inverted_index(s)) # Saving all the values from the inverted index into a list\n",
    "            \n",
    "        for i in inv_ind: # Iterating through all the values in the inverted index in the list\n",
    "            for t in i: # Iterating through al the terms in the list of collections of terms per document\n",
    "                if t == q: # If term is equal to the search query\n",
    "                    return q, i[t] # The term and their respective inverted index document_ids are returned\n",
    "    else:\n",
    "        return 'Wrong method entry, try again!'\n",
    "    \n",
    "print(get_ranked_documents(sample_sentences, 'programming', method_name='tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
