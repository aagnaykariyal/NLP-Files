{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to first try to implement some NLP techniques manually, then we are going check available libraries that perform the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:04:48.166558Z",
     "start_time": "2023-11-03T17:04:48.159591Z"
    }
   },
   "outputs": [],
   "source": [
    "paragraph = [\"Natural ( Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language.\",\n",
    "\"To do this, we often # break down sentences or texts into smaller units called tokens.\",\n",
    "\"These tokens can be as @simple as individual words or even sub-word units.\",\n",
    "\"Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively.\",\n",
    "\"Tokenization would split sentences into individual words.\",\n",
    "\"This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation.\",\n",
    "\"Understanding tokenization is the first step in unlocking the power of NLP.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:45:29.395629Z",
     "start_time": "2023-11-03T16:45:29.389418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing:\n",
      "\n",
      "Natural  Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language. \n",
      "\n",
      "To do this, we often  break down sentences or texts into smaller units called tokens. \n",
      "\n",
      "These tokens can be as simple as individual words or even sub-word units. \n",
      "\n",
      "Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively. \n",
      "\n",
      "Tokenization would split sentences into individual words. \n",
      "\n",
      "This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation. \n",
      "\n",
      "Understanding tokenization is the first step in unlocking the power of NLP                             . \n",
      "\n",
      "After removing: \n",
      "\n",
      "Natural  Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language. \n",
      "\n",
      "To do this, we often  break down sentences or texts into smaller units called tokens. \n",
      "\n",
      "These tokens can be as simple as individual words or even sub-word units. \n",
      "\n",
      "Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively. \n",
      "\n",
      "Tokenization would split sentences into individual words. \n",
      "\n",
      "This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation. \n",
      "\n",
      "Understanding tokenization is the first step in unlocking the power of NLP                                                          . \n"
     ]
    }
   ],
   "source": [
    "# First we need to create a list of special characters that may be encountered in the paragraph\n",
    "special_characters = ['!','\"','#','$','%','&','_','(',')','*','+','/',':','','<','=','>','@','[','\\\\',']','^','`','{','|','}','~','\\t', '\\s']\n",
    "# TODO: Then we need to remove the special characters \n",
    "print('Before Removing:\\n')\n",
    "for i in paragraph:\n",
    "    print(i,\"\\n\")\n",
    "\n",
    "for i in special_characters:\n",
    "    for j in range(len(paragraph)):\n",
    "        paragraph[j] = paragraph[j].replace(i,'')\n",
    "    # Adding space before . and , so that when we split on the spaces\n",
    "    # . and , will be considered as tokens\n",
    "    paragraph[j] = paragraph[j].replace('.', ' .')\n",
    "    paragraph[j] = paragraph[j].replace(',', ' ,')\n",
    "        \n",
    "print(\"After removing: \\n\")\n",
    "for i in paragraph:\n",
    "    print(i,\"\\n\")\n",
    "\n",
    "# Should we remove the dots and commas or include it as a token alone? DISCUSS AND APPLY!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:35:29.287041Z",
     "start_time": "2023-11-03T16:35:29.283627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['Natural',\n  'Language',\n  'Processing,',\n  'or',\n  'NLP,',\n  'is',\n  'a',\n  'fascinating',\n  'field',\n  'that',\n  'focuses',\n  'on',\n  'enabling',\n  'computers',\n  'to',\n  'understand',\n  'and',\n  'work',\n  'with',\n  'human',\n  'language.'],\n ['To',\n  'do',\n  'this,',\n  'we',\n  'often',\n  'break',\n  'down',\n  'sentences',\n  'or',\n  'texts',\n  'into',\n  'smaller',\n  'units',\n  'called',\n  'tokens.'],\n ['These',\n  'tokens',\n  'can',\n  'be',\n  'as',\n  'simple',\n  'as',\n  'individual',\n  'words',\n  'or',\n  'even',\n  'sub-word',\n  'units.'],\n ['Tokenization',\n  'is',\n  'a',\n  'crucial',\n  'step',\n  'in',\n  'NLP',\n  'because',\n  'it',\n  'allows',\n  'us',\n  'to',\n  'analyze',\n  'and',\n  'process',\n  'text',\n  'data',\n  'more',\n  'effectively.'],\n ['Tokenization',\n  'would',\n  'split',\n  'sentences',\n  'into',\n  'individual',\n  'words.'],\n ['This',\n  'breakdown',\n  'forms',\n  'the',\n  'foundation',\n  'for',\n  'various',\n  'NLP',\n  'tasks,',\n  'from',\n  'sentiment',\n  'analysis',\n  'to',\n  'machine',\n  'translation.'],\n ['Understanding',\n  'tokenization',\n  'is',\n  'the',\n  'first',\n  'step',\n  'in',\n  'unlocking',\n  'the',\n  'power',\n  'of',\n  'NLP',\n  '.']]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Now we need to change sentences to tokens\n",
    "token_list = [sentence.split(' ') for sentence in paragraph]\n",
    "token_list = [[tokens for tokens in sentence_token if tokens != ''] for sentence_token in token_list]\n",
    "token_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to discuss one of the most known NLP Library, NLTK. <br>\n",
    "First we need to install NLTK Library:<br>\n",
    "Run - pip install nltk - in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:39:36.745768Z",
     "start_time": "2023-11-03T16:39:33.253103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting click\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.9/97.9 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: joblib in ./venv/lib/python3.11/site-packages (from nltk) (1.3.2)\r\n",
      "Collecting regex>=2021.8.3\r\n",
      "  Downloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m291.0/291.0 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tqdm\r\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tqdm, regex, click, nltk\r\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK: It is a python library that can we used to perform all the NLP tasks(stemming, lemmatization, etc..)<br>\n",
    "In this class, we are going to learn how to do the following: <br>\n",
    "- Tokenization\n",
    "- Stop-word removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Finding Synonym/Antonym\n",
    "- POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T16:40:49.505624Z",
     "start_time": "2023-11-03T16:40:47.981818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Tokenization: \n",
    "Tokenization as discussed previously in class, is the process of dividing the whole text into tokens.<br>\n",
    "There are two main types of Tokenizers available in nltk library:<br>\n",
    "1. Sentence Tokenizer: Takes as an input a string (document) and returns a list of the sentences from the string, where each sentence is considered a token.\n",
    "2. Word Tokenizer: Takes as an input a string (document) and returns a list of words that make up the string, where each word is considered a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:04:56.881860Z",
     "start_time": "2023-11-03T17:04:56.854540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural ( Language Processing, or NLP, is a fascinating field that focuses on enabling computers to understand and work with human language.', 'To do this, we often # break down sentences or texts into smaller units called tokens.', 'These tokens can be as @simple as individual words or even sub-word units.', 'Tokenization is a crucial step in NLP because it allows us to analyze and process text data more effectively.', 'Tokenization would split sentences into individual words.', 'This breakdown forms the foundation for various NLP tasks, from sentiment analysis to machine translation.', 'Understanding tokenization is the first step in unlocking the power of NLP.']\n",
      "['Natural', '(', 'Language', 'Processing', ',', 'or', 'NLP', ',', 'is', 'a', 'fascinating', 'field', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', 'and', 'work', 'with', 'human', 'language', '.', 'To', 'do', 'this', ',', 'we', 'often', '#', 'break', 'down', 'sentences', 'or', 'texts', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'as', '@', 'simple', 'as', 'individual', 'words', 'or', 'even', 'sub-word', 'units', '.', 'Tokenization', 'is', 'a', 'crucial', 'step', 'in', 'NLP', 'because', 'it', 'allows', 'us', 'to', 'analyze', 'and', 'process', 'text', 'data', 'more', 'effectively', '.', 'Tokenization', 'would', 'split', 'sentences', 'into', 'individual', 'words', '.', 'This', 'breakdown', 'forms', 'the', 'foundation', 'for', 'various', 'NLP', 'tasks', ',', 'from', 'sentiment', 'analysis', 'to', 'machine', 'translation', '.', 'Understanding', 'tokenization', 'is', 'the', 'first', 'step', 'in', 'unlocking', 'the', 'power', 'of', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# Importing needed tokenizer functions\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "test_string=\" \".join(paragraph)\n",
    "print(sent_tokenize(test_string))\n",
    "print(word_tokenize(test_string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Stop-Words Removal:\n",
    "Stopwords as discussed previously in class, are words that are not important in the process of analyzing the data. <br>\n",
    "Example of stop-words could include: and, he, it ...<br>\n",
    "The main task is to remove all stop-words from the text before proceeding with preprocessing.<br>\n",
    "\n",
    "NLTK simplifies this task, by providing a list of stopwords, stored in nltk corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:05:03.073453Z",
     "start_time": "2023-11-03T17:05:03.063185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "# stopwords.fileids()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we remove the stop-words from a tokenized string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:05:05.361729Z",
     "start_time": "2023-11-03T17:05:05.352114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Natural',\n '(',\n 'Language',\n 'Processing',\n ',',\n 'NLP',\n ',',\n 'fascinating',\n 'field',\n 'focuses',\n 'enabling',\n 'computers',\n 'understand',\n 'work',\n 'human',\n 'language',\n '.',\n 'To',\n ',',\n 'often',\n '#',\n 'break',\n 'sentences',\n 'texts',\n 'smaller',\n 'units',\n 'called',\n 'tokens',\n '.',\n 'These',\n 'tokens',\n '@',\n 'simple',\n 'individual',\n 'words',\n 'even',\n 'sub-word',\n 'units',\n '.',\n 'Tokenization',\n 'crucial',\n 'step',\n 'NLP',\n 'allows',\n 'us',\n 'analyze',\n 'process',\n 'text',\n 'data',\n 'effectively',\n '.',\n 'Tokenization',\n 'would',\n 'split',\n 'sentences',\n 'individual',\n 'words',\n '.',\n 'This',\n 'breakdown',\n 'forms',\n 'foundation',\n 'various',\n 'NLP',\n 'tasks',\n ',',\n 'sentiment',\n 'analysis',\n 'machine',\n 'translation',\n '.',\n 'Understanding',\n 'tokenization',\n 'first',\n 'step',\n 'unlocking',\n 'power',\n 'NLP',\n '.']"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_removed = [word for word in word_tokenize(test_string) if word not in stopwords.words('english')]\n",
    "stop_words_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtask that accompanies stop-words removal, is the punctuation removal. As we previously saw how to remove stop-words, we need to remove the punctuation tokens in the text. This can be achieved by the help of string library accessing punctuation attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "# to save each punctuation as a list item, we need to call list()\n",
    "punctuation_list = list(string.punctuation)\n",
    "print(punctuation_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove punctuation, the same block of code will be run for removing stop-words, but now replacing the stop-words list by punctuation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'class',\n",
       " 'covering',\n",
       " 'introduction',\n",
       " 'nltk',\n",
       " 'pretty',\n",
       " 'simple',\n",
       " 'The',\n",
       " 'class',\n",
       " 'easy',\n",
       " 'Students',\n",
       " 'enjoy']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_punctuation_removed = [word for word in stop_words_removed if word not in punctuation_list]\n",
    "stop_words_punctuation_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional task that accompanies stop-words removal and punctuation removal, is the Case-Folding. To Case-Fold our list for normalization it is pretty simple, we just need to call string.lower on every word as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'class',\n",
       " 'covering',\n",
       " 'introduction',\n",
       " 'nltk',\n",
       " 'pretty',\n",
       " 'simple',\n",
       " 'the',\n",
       " 'class',\n",
       " 'easy',\n",
       " 'students',\n",
       " 'enjoy']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_punctuation_removed_case_folded = [word.lower() for word in stop_words_punctuation_removed]\n",
    "stop_words_punctuation_removed_case_folded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming: \n",
    "Stemming as discussed in class, is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. <br>\n",
    "In simple words, we can say that stemming is the process of removing plural and adjectives from the word.<br>\n",
    "Stemming is made available in the NLTK Library, with the most used stemmer called PorterStemmer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:19:19.413903Z",
     "start_time": "2023-11-03T17:19:18.669257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/aagnaykariyal/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you had an error of treebank not found, use this command and search in corpora for treebank then download\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:19:21.747410Z",
     "start_time": "2023-11-03T17:19:21.735094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Original ******************************\n",
      "Pierre Vinken , 61 years old , will join the board as a nonexecutive\n",
      "director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch\n",
      "publishing group . Rudolph Agnew , 55 years old and former chairman of\n",
      "Consolidated Gold Fields PLC , was named *-1 a nonexecutive director\n",
      "of this British industrial conglomerate . A form of asbestos once used\n",
      "* * to make Kent cigarette filters has caused a high percentage of\n",
      "cancer deaths among a group of workers exposed * to it more than 30\n",
      "years ago , researchers reported 0 *T*-1 . The asbestos fiber ,\n",
      "crocidolite , is unusually resilient once it enters the lungs , with\n",
      "even brief exposures to it causing symptoms that *T*-1 show up decades\n",
      "later , researchers said 0 *T*-2 . Lorillard Inc. , the unit of New\n",
      "York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped\n",
      "using crocidolite in its Micronite cigarette filters in 1956 .\n",
      "Although preliminary findings were reported *-2 more than a year ago ,\n",
      "the latest results appear in today 's New England Journal of Medicine\n",
      ", a forum likely * to bring new attention to the problem . A Lorillard\n",
      "spokewoman said , `` This is an old story . We 're talking about years\n",
      "ago before anyone heard of asbestos having any questionable properties\n",
      ". There is no asbestos in our products now . '' Neither Lorillard nor\n",
      "the researchers who *T*-3 studied the workers were aware of any\n",
      "research on smokers of the Kent cigarettes . `` We have no useful\n",
      "information on whether users are at risk , '' said *T*-1 James A.\n",
      "Talcott of Boston 's Dana-Farber Cancer Institute . Dr. Talcott led a\n",
      "team of researchers from the National Cancer Institute and the medical\n",
      "schools of Harvard University and Boston University . The Lorillard\n",
      "spokeswoman said 0 asbestos was used *-1 in `` very modest amounts ''\n",
      "in * making paper for the filters in the early 1950s and replaced *-1\n",
      "with a different type of filter in 1956 . From 1953 to 1955 , 9.8\n",
      "billion Kent cigarettes with the filters were sold *-3 , the company\n",
      "said 0 *T*-1 . Among 33 men who *T*-4 worked closely with the\n",
      "substance , 28 *ICH*-1 have died -- more than three times the expected\n",
      "number . Four of the five surviving workers have asbestos-related\n",
      "diseases , including three with recently diagnosed cancer . The total\n",
      "of 18 deaths from malignant mesothelioma , lung cancer and asbestosis\n",
      "was far higher than * expected *?* , the researchers said 0 *T*-1 . ``\n",
      "The morbidity rate is a striking finding among those of us who *T*-5\n",
      "study asbestos-related diseases , '' said *T*-1 Dr. Talcott . The\n",
      "percentage of lung cancer deaths among the workers at the West Groton\n",
      ", Mass. , paper factory appears *-1 to be the highest for any asbestos\n",
      "workers studied * in Western industrialized countries , he said 0\n",
      "*T*-2 . The plant , which *T*-1 is owned *-4 by Hollingsworth & Vose\n",
      "Co. , was under contract *ICH*-2 with Lorillard * to make the\n",
      "cigarette filters . The finding probably will support those who *T*-6\n",
      "argue that the U.S. should regulate the class of asbestos including\n",
      "crocidolite more stringently than the common kind of asbestos ,\n",
      "chrysotile , found * in most schools and other buildings , Dr. Talcott\n",
      "said 0 *T*-1 . The U.S. is one of the few industrialized nations that\n",
      "*T*-7 does n't have a higher standard of regulation for the smooth ,\n",
      "needle-like fibers such as crocidolite that *T*-1 are classified *-5\n",
      "as amphobiles , according to Brooke T. Mossman , a professor of\n",
      "pathlogy at the University of Vermont College of Medicine . More\n",
      "common chrysotile fibers are curly and are more easily rejected *-1 by\n",
      "the body , Dr. Mossman explained 0 *T*-2 . In July , the Environmental\n",
      "Protection Agency imposed a gradual ban on virtually all uses of\n",
      "asbestos . By 1997 , almost all remaining uses of cancer-causing\n",
      "asbestos will be outlawed *-6 . About 160 workers at a factory that\n",
      "*T*-8 made paper for the Kent filters were exposed *-7 to asbestos in\n",
      "the 1950s . Areas of the factory *ICH*-2 were particularly dusty where\n",
      "the crocidolite was used *-8 *T*-1 . Workers dumped large burlap sacks\n",
      "of the imported material into a huge bin , poured in cotton and\n",
      "acetate fibers and mechanically mixed the dry fibers in a process used\n",
      "* * to make filters . Workers described `` clouds of blue dust '' that\n",
      "*T*-1 hung over parts of the factory , even though exhaust fans\n",
      "ventilated the area . `` There 's no question that some of those\n",
      "workers and managers contracted asbestos-related diseases , '' said\n",
      "*T*-1 Darrell Phillips , vice president of human resources for\n",
      "Hollingsworth & Vose . `` But you have *-1 to recognize that these\n",
      "events took place 35 years ago . It has no bearing on our work force\n",
      "today .\n",
      "****************************** Results *******************************\n",
      "pierr vinken , 61 year old , will join the board as a nonexecut\n",
      "director nov. 29 . mr. vinken is chairman of elsevi n.v. , the dutch\n",
      "publish group . rudolph agnew , 55 year old and former chairman of\n",
      "consolid gold field plc , wa name *-1 a nonexecut director of thi\n",
      "british industri conglomer . a form of asbesto onc use * * to make\n",
      "kent cigarett filter ha caus a high percentag of cancer death among a\n",
      "group of worker expos * to it more than 30 year ago , research report\n",
      "0 *t*-1 . the asbesto fiber , crocidolit , is unusu resili onc it\n",
      "enter the lung , with even brief exposur to it caus symptom that *t*-1\n",
      "show up decad later , research said 0 *t*-2 . lorillard inc. , the\n",
      "unit of new york-bas loew corp. that *t*-2 make kent cigarett , stop\n",
      "use crocidolit in it micronit cigarett filter in 1956 . although\n",
      "preliminari find were report *-2 more than a year ago , the latest\n",
      "result appear in today 's new england journal of medicin , a forum\n",
      "like * to bring new attent to the problem . a lorillard spokewoman\n",
      "said , `` thi is an old stori . we 're talk about year ago befor anyon\n",
      "heard of asbesto have ani question properti . there is no asbesto in\n",
      "our product now . '' neither lorillard nor the research who *t*-3\n",
      "studi the worker were awar of ani research on smoker of the kent\n",
      "cigarett . `` we have no use inform on whether user are at risk , ''\n",
      "said *t*-1 jame a. talcott of boston 's dana-farb cancer institut .\n",
      "dr. talcott led a team of research from the nation cancer institut and\n",
      "the medic school of harvard univers and boston univers . the lorillard\n",
      "spokeswoman said 0 asbesto wa use *-1 in `` veri modest amount '' in *\n",
      "make paper for the filter in the earli 1950 and replac *-1 with a\n",
      "differ type of filter in 1956 . from 1953 to 1955 , 9.8 billion kent\n",
      "cigarett with the filter were sold *-3 , the compani said 0 *t*-1 .\n",
      "among 33 men who *t*-4 work close with the substanc , 28 *ich*-1 have\n",
      "die -- more than three time the expect number . four of the five\n",
      "surviv worker have asbestos-rel diseas , includ three with recent\n",
      "diagnos cancer . the total of 18 death from malign mesothelioma , lung\n",
      "cancer and asbestosi wa far higher than * expect *?* , the research\n",
      "said 0 *t*-1 . `` the morbid rate is a strike find among those of us\n",
      "who *t*-5 studi asbestos-rel diseas , '' said *t*-1 dr. talcott . the\n",
      "percentag of lung cancer death among the worker at the west groton ,\n",
      "mass. , paper factori appear *-1 to be the highest for ani asbesto\n",
      "worker studi * in western industri countri , he said 0 *t*-2 . the\n",
      "plant , which *t*-1 is own *-4 by hollingsworth & vose co. , wa under\n",
      "contract *ich*-2 with lorillard * to make the cigarett filter . the\n",
      "find probabl will support those who *t*-6 argu that the u.s. should\n",
      "regul the class of asbesto includ crocidolit more stringent than the\n",
      "common kind of asbesto , chrysotil , found * in most school and other\n",
      "build , dr. talcott said 0 *t*-1 . the u.s. is one of the few industri\n",
      "nation that *t*-7 doe n't have a higher standard of regul for the\n",
      "smooth , needle-lik fiber such as crocidolit that *t*-1 are classifi\n",
      "*-5 as amphobil , accord to brook t. mossman , a professor of pathlog\n",
      "at the univers of vermont colleg of medicin . more common chrysotil\n",
      "fiber are curli and are more easili reject *-1 by the bodi , dr.\n",
      "mossman explain 0 *t*-2 . in juli , the environment protect agenc\n",
      "impos a gradual ban on virtual all use of asbesto . by 1997 , almost\n",
      "all remain use of cancer-caus asbesto will be outlaw *-6 . about 160\n",
      "worker at a factori that *t*-8 made paper for the kent filter were\n",
      "expos *-7 to asbesto in the 1950 . area of the factori *ich*-2 were\n",
      "particularli dusti where the crocidolit wa use *-8 *t*-1 . worker dump\n",
      "larg burlap sack of the import materi into a huge bin , pour in cotton\n",
      "and acet fiber and mechan mix the dri fiber in a process use * * to\n",
      "make filter . worker describ `` cloud of blue dust '' that *t*-1 hung\n",
      "over part of the factori , even though exhaust fan ventil the area .\n",
      "`` there 's no question that some of those worker and manag contract\n",
      "asbestos-rel diseas , '' said *t*-1 darrel phillip , vice presid of\n",
      "human resourc for hollingsworth & vose . `` but you have *-1 to recogn\n",
      "that these event took place 35 year ago . it ha no bear on our work\n",
      "forc today .\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# a demo of sample input output to Porter Stemmer\n",
    "nltk.stem.porter.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:19:26.869175Z",
     "start_time": "2023-11-03T17:19:26.846718Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words_punctuation_removed_case_folded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m stemmer \u001B[38;5;241m=\u001B[39m PorterStemmer()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# lets use it to stem the words we had from before\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m stemmed_words \u001B[38;5;241m=\u001B[39m [stemmer\u001B[38;5;241m.\u001B[39mstem(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mstop_words_punctuation_removed_case_folded\u001B[49m]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(stop_words_punctuation_removed_case_folded, stemmed_words)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stop_words_punctuation_removed_case_folded' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# create an instance of the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# lets use it to stem the words we had from before\n",
    "stemmed_words = [stemmer.stem(word) for word in stop_words_punctuation_removed_case_folded]\n",
    "print(stop_words_punctuation_removed_case_folded, stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization: \n",
    "Lemmatization as discussed in class targets to do things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. <br>\n",
    "In simple words lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word.<br>\n",
    "<< Then why to use Stemming? >> <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers are also available in nltk.stem, lets test the use of WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'class', 'covering', 'introduction', 'nltk', 'pretty', 'simple', 'the', 'class', 'easy', 'students', 'enjoy']\n",
      "['this', 'class', 'covering', 'introduction', 'nltk', 'pretty', 'simple', 'the', 'class', 'easy', 'student', 'enjoy']\n",
      "['thi', 'class', 'cover', 'introduct', 'nltk', 'pretti', 'simpl', 'the', 'class', 'easi', 'student', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lets use it to lemmatize the words we had from before, and print the results, stemming and original\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_punctuation_removed_case_folded]\n",
    "print(stop_words_punctuation_removed_case_folded)\n",
    "print(lemmatized_words)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Synonym/Antonym: \n",
    "Finding a synonym or antonym for a word, is useful if you need to identify words that mean the same in different context. For this, nltk corpus provides a wordnet corpus, which is a dictionary for the English Language designed for NLP tasks.<br>\n",
    "Lets check how does it work, by importing wordnet from nltk.corpus and cecking for some synonyms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sorry', 'sad', 'pitiful', 'distressing', 'deplorable', 'lamentable'} {'glad'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym_list = []\n",
    "antonym_list = []\n",
    "test_word = 'sad'\n",
    "for synonym in wordnet.synsets(test_word):\n",
    "    for i in synonym.lemmas(): ## Finding the lemma,matching and then appending synonyms\n",
    "        synonym_list.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonym_list.append(i.antonyms()[0].name())\n",
    "print(set(synonym_list), set(antonym_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging (POS): \n",
    "Stemming as discussed in class, is a process of converting a sentence to forms — a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T17:17:15.540960Z",
     "start_time": "2023-11-03T17:17:15.366640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/aagnaykariyal/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets') #download tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the POS tagging, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('covering', 'VBG'), ('an', 'DT'), ('introduction', 'NN'), ('to', 'TO'), ('nltk', 'VB'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('pretty', 'JJ'), ('simple', 'JJ'), ('.', '.'), ('The', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('easy', 'JJ'), ('!', '.'), ('Students', 'NNS'), ('will', 'MD'), ('enjoy', 'VB'), ('it', 'PRP'), ('.', '.')]\n",
      "[('this', 'DT'), ('class', 'NN'), ('covering', 'VBG'), ('introduction', 'NN'), ('nltk', 'FW'), ('pretty', 'RB'), ('simple', 'JJ'), ('the', 'DT'), ('class', 'NN'), ('easy', 'JJ'), ('student', 'NN'), ('enjoy', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# We apply POS tagging to words before any preprocessing, check the difference\n",
    "print(nltk.pos_tag(word_tokenize(test_string)))\n",
    "print(nltk.pos_tag(word_tokenize(\" \".join(lemmatized_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
