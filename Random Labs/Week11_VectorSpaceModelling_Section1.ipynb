{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 11: Vector Space Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through a simple example of Vector Space Modelling. Then we will use cosine similarity to find similarity between document and query and rank the documents accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = ['The quick brown fox jumps over the lazy dog.',\n",
    "               'A brown dog chased the fox.',\n",
    "               'The dog is lazy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'],\n",
       " ['A', 'brown', 'dog', 'chased', 'the', 'fox', '.'],\n",
       " ['The', 'dog', 'is', 'lazy', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First step is to tokenize our text\n",
    "tokenized_documents = [word_tokenize(document) for document in sample_docs]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.'],\n",
       " ['brown', 'dog', 'chased', 'fox', '.'],\n",
       " ['dog', 'lazy', '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Second step is to calculate our TF-IDF \n",
    "## We need first to preprocess our text\n",
    "## For simplicity I will just remove the stop words in documents\n",
    "## and I will change words to lower\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "cleaned_data = [[word.lower() for word in document if word.lower() not in english_stopwords] for document in tokenized_documents]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick brown fox jumps lazy dog .', 'brown dog chased fox .', 'dog lazy .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TF_IDF vectorizer takes as an input sentences, lets join our tokens\n",
    "cleaned_sentences = [' '.join(document) for document in cleaned_data]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.29225439586501756\n",
      "  (0, 5)\t0.37633074615060896\n",
      "  (0, 4)\t0.49482970636510465\n",
      "  (0, 3)\t0.37633074615060896\n",
      "  (0, 0)\t0.37633074615060896\n",
      "  (0, 6)\t0.49482970636510465\n",
      "  (1, 1)\t0.6317450542765208\n",
      "  (1, 2)\t0.3731188059313277\n",
      "  (1, 3)\t0.4804583972923858\n",
      "  (1, 0)\t0.4804583972923858\n",
      "  (2, 2)\t0.6133555370249717\n",
      "  (2, 5)\t0.7898069290660905\n"
     ]
    }
   ],
   "source": [
    "## Lets define our vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_sentences)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown dog']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Given that we have the TFIDF vectors, lets write the query and then get the vector of the query\n",
    "query = \"the brown dog\"\n",
    "## Preprocess the query\n",
    "query_tokens = word_tokenize(query)\n",
    "query_cleaned = [word.lower() for word in query_tokens if word.lower() not in english_stopwords]\n",
    "query_cleaned_combined = [' '.join(query_cleaned)]\n",
    "query_cleaned_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.6133555370249717\n",
      "  (0, 0)\t0.7898069290660905\n"
     ]
    }
   ],
   "source": [
    "## Get the TFIDF vector of the query\n",
    "query_tfIdf_vector = tfidf_vectorizer.transform(query_cleaned_combined)\n",
    "print(query_tfIdf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47648448, 0.60832386, 0.37620501]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we need to find the cosine similarity between the query and documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(query_tfIdf_vector, tfidf_matrix)\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The quick brown fox jumps over the lazy dog.', 0.4764844828540594),\n",
       " ('A brown dog chased the fox.', 0.6083238568956406),\n",
       " ('The dog is lazy.', 0.37620501479919144)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To rank the documents, first we will create a list of ranked results\n",
    "results = [(sample_docs[i], cosine_similarities[0][i]) for i in range(len(sample_docs))]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A brown dog chased the fox.', 0.6083238568956406),\n",
       " ('The quick brown fox jumps over the lazy dog.', 0.4764844828540594),\n",
       " ('The dog is lazy.', 0.37620501479919144)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sorting the results based on similarity to rank the documents\n",
    "results.sort(key=lambda x:x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On Exercise InClass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following relevant and retrieved documents. Find the precision and recall of this retrieval system.<br>\n",
    "Assume that all documents are either relevant or retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 0.5 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Sample relevant documents and retrieved documents\n",
    "relevant_documents = [0, 1, 2, 4]\n",
    "retrieved_documents = [0, 1, 3, 5, 7]\n",
    "\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "\n",
    "#tp_count = len([num for num in relevant_documents if num in retrieved_documents])\n",
    "# or \n",
    "tp_count = set(relevant_documents) & set(retrieved_documents)\n",
    "fp_count = len([num for num in retrieved_documents if num not in relevant_documents])\n",
    "\n",
    "precision = len(tp_count)/len(retrieved_documents)\n",
    "recall = len(tp_count)/len(relevant_documents)\n",
    "total_num_doc = len(set(relevant_documents) | set(retrieved_documents))\n",
    "accuracy = len(tp_count)/total_num_doc\n",
    "\n",
    "print(precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.6666666666666666, 0.4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate precision at k for the following k_values\n",
    "k_values = [1,3,5]\n",
    "\n",
    "precision_at_k = []\n",
    "# Calculate Precision at k\n",
    "for k in k_values:\n",
    "    retrieved_doc_trunc = retrieved_documents[:k]\n",
    "    \n",
    "    tp_count = set(relevant_documents) & set(retrieved_doc_trunc)\n",
    "    precision = len(tp_count)/len(retrieved_doc_trunc)\n",
    "\n",
    "    precision_at_k.append(precision) \n",
    "\n",
    "precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average precision\n",
    "\n",
    "# first find the k values where you need to calculate the precision at\n",
    "relevant_retrieved = set(relevant_documents) & set(retrieved_documents)\n",
    "indices = [retrieved_documents.index(val)for val in relevant_retrieved]\n",
    "\n",
    "precision_at_k = []\n",
    "# Calculate Precision at k\n",
    "for k in indices:\n",
    "    retrieved_doc_trunc = retrieved_documents[:k +1]\n",
    "    \n",
    "    tp_count = set(relevant_documents) & set(retrieved_doc_trunc)\n",
    "    precision = len(tp_count)/len(retrieved_doc_trunc)\n",
    "\n",
    "    precision_at_k.append(precision) \n",
    "\n",
    "avg_p_at_k = sum(precision_at_k)/len(precision_at_k)\n",
    "precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Mean_Average_Precision Given that another IR System returns the following results\n",
    "retrieved_documents_ir2 = [0, 3, 7, 2]\n",
    "\n",
    "# calculate avg_p_at_k for ir2\n",
    "relevant_retrieved = set(relevant_documents) & set(retrieved_documents_ir2)\n",
    "indices = [retrieved_documents_ir2.index(val)for val in relevant_retrieved]\n",
    "\n",
    "precision_at_k2 = []\n",
    "# Calculate Precision at k\n",
    "for k in indices:\n",
    "    retrieved_doc_trunc = retrieved_documents_ir2[:k +1]\n",
    "    \n",
    "    tp_count = set(relevant_documents) & set(retrieved_doc_trunc)\n",
    "    precision = len(tp_count)/len(retrieved_doc_trunc)\n",
    "\n",
    "    precision_at_k2.append(precision) \n",
    "\n",
    "avg_p_at_k2 = sum(precision_at_k2)/len(precision_at_k2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "## Mean_Average_Precision\n",
    "\n",
    "mean_avg_precision = (avg_p_at_k + avg_p_at_k2)/2\n",
    "print(mean_avg_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following documents and query. Find the cosine_similarity between documents and the query. Rank the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T02:25:22.704216Z",
     "start_time": "2023-11-23T02:25:22.615184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top documents:\n",
      "Data preprocessing is essential for machine learning models.\n",
      "Machine learning algorithms analyze data to make predictions.\n",
      "Natural language processing is a field of computer science.\n",
      "Top documents:\n",
      "Neural networks are used in deep learning models.\n",
      "Machine learning algorithms analyze data to make predictions.\n",
      "Data preprocessing is essential for machine learning models.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural language processing is a field of computer science.\",\n",
    "    \"Machine learning algorithms analyze data to make predictions.\",\n",
    "    \"Data preprocessing is essential for machine learning models.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Information retrieval involves finding relevant information in a collection.\",\n",
    "    \"Neural networks are used in deep learning models.\",\n",
    "    \"Statistical analysis helps in understanding data patterns.\",\n",
    "    \"Big data technologies handle large volumes of data.\",\n",
    "    \"Classification and regression are types of supervised learning.\",\n",
    "    \"Clustering algorithms group similar data points together.\"\n",
    "]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Queries\n",
    "queries = [\n",
    "    \"What is the importance of preprocessing in machine learning?\",\n",
    "    \"How do neural networks contribute to deep learning?\"\n",
    "]\n",
    "# Convert query to TF-IDF representation\n",
    "for query in queries:\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between query and documents\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    results = cosine_similarities[0].argsort()[-3:][::-1]  # Top 3 relevant documents\n",
    "    \n",
    "    # Output top documents\n",
    "    print(\"Top documents:\")\n",
    "    for idx in results:\n",
    "        print(documents[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_actual_ranks = [[1,8,4],[1,2,5]]\n",
    "## Calculate the MAP given the results you got from cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kappa Measure Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator 1's relevance assessments\n",
    "annotator1 = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 indicates relevant, 0 indicates not relevant\n",
    "\n",
    "# Annotator 2's relevance assessments (with some disagreements)\n",
    "annotator2 = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1 indicates relevant, 0 indicates not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.500000000000002"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kappa score of confidence\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(annotator2, annotator1)\n",
    "P_A = (conf_mat[0,0] + conf_mat[1,1])/sum(conf_mat.flatten())\n",
    "\n",
    "# one of these is screwed up somewhere. But the idea is there\n",
    "P_rel = (sum(conf_mat[:,1].flatten())/len(annotator1)) * (sum(conf_mat[1:,].flatten())/len(annotator1))\n",
    "P_norel = (sum(conf_mat[:,0].flatten())/len(annotator1)) * (sum(conf_mat[0:,].flatten())/len(annotator1))\n",
    "\n",
    "P_E = P_rel + P_norel\n",
    "kappa = (P_A + P_E)/(1- P_E)\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for IR: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Compute Cohen's Kappa for relevance assessments\n",
    "kappa_ir = cohen_kappa_score(annotator1, annotator2)\n",
    "\n",
    "print(f\"Cohen's Kappa for IR: {kappa_ir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output suggests a fair agreement between the two annotators on the relevance assessments. More annotators are needed or replace one of the annotators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
